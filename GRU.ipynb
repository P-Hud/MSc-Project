{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f952efb-2257-4979-acfd-c11a71a7040b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Data Manipulation and Preparation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Evaluation metrics and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "# Utility imports\n",
    "import itertools\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Set up GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fd3861-56b0-4b8e-be62-4a237a8d07d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed datasets\n",
    "path_30min = ''\n",
    "path_4h = ''\n",
    "path_1d = ''\n",
    "\n",
    "df_30min = pd.read_csv(path_30min)\n",
    "df_4h = pd.read_csv(path_4h)\n",
    "df_daily = pd.read_csv(path_1d)\n",
    "\n",
    "all_dfs = [df_30min, df_4h, df_daily]\n",
    "\n",
    "# Convert timestamp columns to datetime\n",
    "for df in all_dfs:\n",
    "    df['open_time'] = pd.to_datetime(df['open_time'])\n",
    "    df['close_time'] = pd.to_datetime(df['close_time'])\n",
    "\n",
    "# Define feature sets\n",
    "price_features = ['close', 'high', 'low', 'volume', 'quote_vol', 'count', 'buy_base', 'buy_quote']\n",
    "diff_features = ['close_diff', 'high_diff', 'low_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62921c7e-9e23-49f1-9582-852e928f2e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GRU Model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, gru_dim, dense_dim, output_dim, num_layers=1, dropout=0.0, activation_function=nn.ReLU):\n",
    "        \"\"\"\n",
    "        Initializes the GRU model.\n",
    "        Args:\n",
    "        - input_dim: Number of input features.\n",
    "        - gru_dim: Number of units in the GRU layer.\n",
    "        - dense_dim: Number of units in the dense (fully connected) layer.\n",
    "        - output_dim: Number of output features.\n",
    "        - num_layers: Number of GRU layers.\n",
    "        - dropout: Dropout rate (used if num_layers > 1).\n",
    "        - activation_function: Activation function to use between dense layers.\n",
    "        \"\"\"\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_dim, gru_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc1 = nn.Linear(gru_dim, dense_dim)\n",
    "        self.activation = activation_function()\n",
    "        self.fc2 = nn.Linear(dense_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the model.\n",
    "        Args:\n",
    "        - x: Input tensor of shape (batch_size, sequence_length, input_dim).\n",
    "        Returns:\n",
    "        - Output tensor of shape (batch_size, output_dim).\n",
    "        \"\"\"\n",
    "        x, _ = self.gru(x)\n",
    "        x = x[:, -1, :]  # Take the last output of the GRU sequence\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c127a303-0216-43e4-9b5b-9a2fca56189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sequences for training the model\n",
    "def create_sequences(data, target, window_size):\n",
    "    \"\"\"\n",
    "    Splits the data into sequences for GRU training.\n",
    "    Args:\n",
    "    - data: The feature data (e.g., prices).\n",
    "    - target: The target data (e.g., future prices).\n",
    "    - window_size: Number of time steps in each sequence.\n",
    "    Returns:\n",
    "    - sequences: Array of sequences of feature data.\n",
    "    - labels: Array of corresponding labels (targets).\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(data) - window_size):\n",
    "        seq = data[i:i + window_size]\n",
    "        label = target[i + window_size]\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "# Function to prepare data for training and testing\n",
    "def prepare_data(df, target_column, window_size, feature_columns):\n",
    "    \"\"\"\n",
    "    Prepares the data for training and testing the GRU model.\n",
    "    Args:\n",
    "    - df: The dataframe containing the data.\n",
    "    - target_column: The name of the column to be predicted.\n",
    "    - window_size: Number of time steps in each input sequence.\n",
    "    - feature_columns: List of columns to use as features.\n",
    "    Returns:\n",
    "    - X_train_seq: Sequences for training.\n",
    "    - X_test_seq: Sequences for testing.\n",
    "    - y_train_seq: Corresponding labels for training sequences.\n",
    "    - y_test_seq: Corresponding labels for testing sequences.\n",
    "    - scaler_y: Scaler used for the target column.\n",
    "    \"\"\"\n",
    "    X = df[feature_columns].values\n",
    "    y = df[target_column].values.reshape(-1, 1)\n",
    "    \n",
    "    scaler_X = MinMaxScaler()\n",
    "    X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "    scaler_y = MinMaxScaler()\n",
    "    y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, shuffle=False)\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train, y_train, window_size)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test, y_test, window_size)\n",
    "\n",
    "    return X_train_seq, X_test_seq, y_train_seq, y_test_seq, scaler_y\n",
    "\n",
    "# Helper function to plot and save the training and validation loss over epochs\n",
    "def plot_and_save_loss(training_loss, validation_loss, file_prefix, model_counter, start_epoch=3):\n",
    "    \"\"\"\n",
    "    Plots and saves the training and validation loss over epochs.\n",
    "    Args:\n",
    "    - training_loss: List of training losses per epoch.\n",
    "    - validation_loss: List of validation losses per epoch.\n",
    "    - file_prefix: Prefix for the saved file names.\n",
    "    - model_counter: Model identifier (used in file names).\n",
    "    - start_epoch: Epoch to start plotting (useful to skip early high losses).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(start_epoch, len(training_loss)), training_loss[start_epoch:], label='Training Loss')\n",
    "    plt.plot(range(start_epoch, len(validation_loss)), validation_loss[start_epoch:], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Model {model_counter} - Training and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{file_prefix}_loss_plot_model_{model_counter}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Save losses to a CSV file\n",
    "    loss_df = pd.DataFrame({\n",
    "        \"Epoch\": range(len(training_loss)),\n",
    "        \"Training Loss\": training_loss,\n",
    "        \"Validation Loss\": validation_loss\n",
    "    })\n",
    "    loss_file_path = f'{file_prefix}_losses_model_{model_counter}.csv'\n",
    "    loss_df.to_csv(loss_file_path, index=False)\n",
    "\n",
    "# Function to save the trained model to a file\n",
    "def save_model(model, file_path):\n",
    "    \"\"\"\n",
    "    Saves the trained PyTorch model to the specified file path.\n",
    "    Args:\n",
    "    - model: The trained model.\n",
    "    - file_path: Path where the model will be saved.\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), file_path)\n",
    "\n",
    "# Function to save predictions to CSV files for further analysis\n",
    "def save_predictions(y_train, y_train_pred, y_test, y_test_pred, model_counter, file_prefix):\n",
    "    \"\"\"\n",
    "    Saves the actual and predicted values for training and testing data to CSV files.\n",
    "    Args:\n",
    "    - y_train: Actual training labels.\n",
    "    - y_train_pred: Predicted training labels.\n",
    "    - y_test: Actual testing labels.\n",
    "    - y_test_pred: Predicted testing labels.\n",
    "    - model_counter: Model identifier (used in file names).\n",
    "    - file_prefix: Prefix for the saved file names.\n",
    "    \"\"\"\n",
    "    train_df = pd.DataFrame({\"Actual\": y_train.flatten(), f\"Model_{model_counter}\": y_train_pred.flatten()})\n",
    "    test_df = pd.DataFrame({\"Actual\": y_test.flatten(), f\"Model_{model_counter}\": y_test_pred.flatten()})\n",
    "\n",
    "    train_file_path = f\"{file_prefix}_train_predictions_model_{model_counter}.csv\"\n",
    "    test_file_path = f\"{file_prefix}_test_predictions_model_{model_counter}.csv\"\n",
    "    train_df.to_csv(train_file_path, index=False)\n",
    "    test_df.to_csv(test_file_path, index=False)\n",
    "\n",
    "# Main function to train and evaluate the GRU model\n",
    "def train_and_evaluate(df, target_column, feature_columns, params, model_counter, file_prefix='results', use_early_stopping=False, plot_loss=False):\n",
    "    \"\"\"\n",
    "    Trains and evaluates the GRU model using the provided parameters and data.\n",
    "    Args:\n",
    "    - df: DataFrame containing the data.\n",
    "    - target_column: The name of the column to predict.\n",
    "    - feature_columns: List of feature columns.\n",
    "    - params: Dictionary of hyperparameters for the model.\n",
    "    - model_counter: Identifier for the model (used in file names and logs).\n",
    "    - file_prefix: Prefix for saving results and models.\n",
    "    - use_early_stopping: Boolean flag to use early stopping during training.\n",
    "    - plot_loss: Boolean flag to plot and save the loss over epochs.\n",
    "    Returns:\n",
    "    - result: Dictionary containing performance metrics and model details.\n",
    "    \"\"\"\n",
    "    result = {}  # Initialize result to ensure it's always defined\n",
    "    try:\n",
    "        # Extract parameters\n",
    "        window_size = params['window_size']\n",
    "        gru_dim = params['gru_dim'] \n",
    "        dense_dim = params['dense_dim']\n",
    "        num_layers = params['num_layers']\n",
    "        dropout = params['dropout']\n",
    "        lr = params['lr']\n",
    "        batch_size = params['batch_size']\n",
    "        num_epochs = params['num_epochs']\n",
    "        optimizer_type = params['optimizer_type']\n",
    "        patience = params['patience']\n",
    "        activation_function = params['activation_function']\n",
    "\n",
    "        # Clear CUDA cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Prepare data sequences for training and testing\n",
    "        X_train_seq, X_test_seq, y_train_seq, y_test_seq, scaler_y = prepare_data(df, target_column, window_size, feature_columns)\n",
    "\n",
    "        # Convert sequences to PyTorch tensors and move to the device (GPU)\n",
    "        X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32).to(device)\n",
    "        X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32).to(device)\n",
    "        y_train_tensor = torch.tensor(y_train_seq, dtype=torch.float32).view(-1, 1).to(device)\n",
    "        y_test_tensor = torch.tensor(y_test_seq, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "        # Create DataLoader objects for training and testing\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Initialize the GRU model with the specified parameters\n",
    "        input_dim = X_train_tensor.shape[2]\n",
    "        output_dim = 1\n",
    "        model = GRUModel(input_dim, gru_dim, dense_dim, output_dim, num_layers, dropout, activation_function).to(device)\n",
    "\n",
    "        # Define the loss function and the optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = getattr(optim, optimizer_type)(model.parameters(), lr=lr)\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        training_losses = []\n",
    "        validation_losses = []\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            epoch_train_loss = 0\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_train_loss += loss.item()\n",
    "\n",
    "            # Calculate average training loss for the epoch\n",
    "            epoch_train_loss /= len(train_loader)\n",
    "            training_losses.append(epoch_train_loss)\n",
    "\n",
    "            # Evaluate the model on the validation (test) set\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in test_loader:\n",
    "                    outputs = model(inputs)\n",
    "                    val_loss += criterion(outputs, labels).item()\n",
    "\n",
    "            # Calculate average validation loss for the epoch\n",
    "            val_loss /= len(test_loader)\n",
    "            validation_losses.append(val_loss)\n",
    "\n",
    "        # Calculate total training time\n",
    "        training_time = time.time() - start_time\n",
    "\n",
    "        # Final evaluation on the training and testing data\n",
    "        model.eval()\n",
    "        evaluation_start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            train_predictions = model(X_train_tensor).cpu().numpy()\n",
    "            test_predictions = model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "            # Inverse transform predictions and actual values back to the original scale\n",
    "            train_predictions_inverse = scaler_y.inverse_transform(train_predictions)\n",
    "            test_predictions_inverse = scaler_y.inverse_transform(test_predictions)\n",
    "            y_train_inverse = scaler_y.inverse_transform(y_train_seq)\n",
    "            y_test_inverse = scaler_y.inverse_transform(y_test_seq)\n",
    "\n",
    "            # Calculate performance metrics\n",
    "            train_mse = mean_squared_error(y_train_inverse, train_predictions_inverse)\n",
    "            test_mse = mean_squared_error(y_test_inverse, test_predictions_inverse)\n",
    "            train_mae = mean_absolute_error(y_train_inverse, train_predictions_inverse)\n",
    "            test_mae = mean_absolute_error(y_test_inverse, test_predictions_inverse)\n",
    "            train_rmse = np.sqrt(train_mse)\n",
    "            test_rmse = np.sqrt(test_mse)\n",
    "            train_r2 = r2_score(y_train_inverse, train_predictions_inverse)\n",
    "            test_r2 = r2_score(y_test_inverse, test_predictions_inverse)\n",
    "            train_mape = np.mean(np.abs((y_train_inverse - train_predictions_inverse) / y_train_inverse)) * 100\n",
    "            test_mape = np.mean(np.abs((y_test_inverse - test_predictions_inverse) / y_test_inverse)) * 100\n",
    "            train_directional_acc = np.mean(np.sign(y_train_inverse[1:] - y_train_inverse[:-1]) == np.sign(train_predictions_inverse[1:] - train_predictions_inverse[:-1]))\n",
    "            test_directional_acc = np.mean(np.sign(y_test_inverse[1:] - y_test_inverse[:-1]) == np.sign(test_predictions_inverse[1:] - test_predictions_inverse[:-1]))\n",
    "\n",
    "        evaluation_time = time.time() - evaluation_start_time\n",
    "\n",
    "        # Store results in a dictionary\n",
    "        result = {\n",
    "            \"window_size\": window_size,\n",
    "            \"gru_dim\": gru_dim,  # Changed from lstm_dim to gru_dim\n",
    "            \"num_layers\": num_layers,\n",
    "            \"dense_dim\": dense_dim,\n",
    "            \"dropout\": dropout,\n",
    "            \"lr\": lr,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"optimizer_type\": optimizer_type,\n",
    "            \"train_mse\": train_mse,\n",
    "            \"test_mse\": test_mse,\n",
    "            \"train_mae\": train_mae,\n",
    "            \"test_mae\": test_mae,\n",
    "            \"train_rmse\": train_rmse,\n",
    "            \"test_rmse\": test_rmse,\n",
    "            \"train_r2\": train_r2,\n",
    "            \"test_r2\": test_r2,\n",
    "            \"train_mape\": train_mape,\n",
    "            \"test_mape\": test_mape,\n",
    "            \"train_directional_acc\": train_directional_acc,\n",
    "            \"test_directional_acc\": test_directional_acc,\n",
    "            \"training_time\": training_time,\n",
    "            \"evaluation_time\": evaluation_time,\n",
    "            \"patience\": patience,\n",
    "            \"activation_function\": activation_function.__name__\n",
    "        }\n",
    "\n",
    "        # Save the result to a CSV file\n",
    "        with open(f'{file_prefix}_{target_column}.csv', 'a', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=result.keys())\n",
    "            if f.tell() == 0:\n",
    "                writer.writeheader()  # Write header only if file is empty\n",
    "            writer.writerow(result)\n",
    "\n",
    "        # Save predictions for further analysis\n",
    "        save_predictions(y_train_inverse, train_predictions_inverse, y_test_inverse, test_predictions_inverse, model_counter, file_prefix)\n",
    "\n",
    "        # Plot and save loss over epochs if specified\n",
    "        if plot_loss:\n",
    "            plot_and_save_loss(training_losses, validation_losses, file_prefix, model_counter)\n",
    "\n",
    "        # Print a summary of the results\n",
    "        print(f\"Results: Train MAE: {train_mae:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "        print(f\"Train Directional Accuracy: {train_directional_acc:.4f}, Test Directional Accuracy: {test_directional_acc:.4f}\")\n",
    "        print(f\"Training Time: {training_time:.4f} seconds, Evaluation Time: {evaluation_time:.4f} seconds\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle exceptions and store error details\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        result = {\n",
    "            \"window_size\": window_size,\n",
    "            \"gru_dim\": gru_dim,\n",
    "            \"num_layers\": num_layers,\n",
    "            \"dense_dim\": dense_dim,\n",
    "            \"dropout\": dropout,\n",
    "            \"lr\": lr,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"optimizer_type\": optimizer_type,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8fc506-a1fd-4a47-86d8-0faa6e38f5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter spaces for grid search\n",
    "params_daily = {\n",
    "    'window_size': [1],\n",
    "    'gru_dim': [60, 80, 120],\n",
    "    'dense_dim': [60, 80, 120],\n",
    "    'num_layers': [1],\n",
    "    'dropout': [0.0],\n",
    "    'lr': [0.0001],\n",
    "    'num_epochs': [40, 50, 60, 70],\n",
    "    'batch_size': [64],\n",
    "    'optimizer_type': ['Adam'],\n",
    "    'patience': [24],\n",
    "    'activation_function': [nn.Tanh]\n",
    "}\n",
    "\n",
    "params_30min = {\n",
    "    'window_size': [48],\n",
    "    'gru_dim': [80, 90, 100],\n",
    "    'dense_dim': [90, 100, 110],\n",
    "    'num_layers': [1],\n",
    "    'dropout': [0.0],\n",
    "    'lr': [0.0001],\n",
    "    'num_epochs': [60, 70, 80, 90],\n",
    "    'batch_size': [48],\n",
    "    'optimizer_type': ['Adam'],\n",
    "    'patience': [24],\n",
    "    'activation_function': [nn.Tanh]\n",
    "}\n",
    "\n",
    "params_4h = {\n",
    "    'window_size': [6],\n",
    "    'gru_dim': [100, 120, 140],\n",
    "    'dense_dim': [90, 100, 110],\n",
    "    'num_layers': [1],\n",
    "    'dropout': [0.0],\n",
    "    'lr': [0.00001],\n",
    "    'num_epochs': [40, 50, 60, 70],\n",
    "    'batch_size': [64],\n",
    "    'optimizer_type': ['Adam'],\n",
    "    'patience': [24],\n",
    "    'activation_function': [nn.Tanh]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2537a8d3-2384-4c3d-ae68-13c69b006b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_grid_search(df_daily, 'df_daily', params_4h)\n",
    "run_grid_search(df_4h, 'df 4h', params_4h)\n",
    "run_grid_search(df_30min, 'df_30min', params_4h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
