{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d35a0f-5f05-4a4b-9eeb-c7dce21c368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1543ba95-2019-4109-9712-b31ccbe9f542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed datasets\n",
    "path_30min = ''\n",
    "path_4h = ''\n",
    "path_1d = ''\n",
    "\n",
    "df_30min = pd.read_csv(path_30min)\n",
    "df_4h = pd.read_csv(path_4h)\n",
    "df_daily = pd.read_csv(path_1d)\n",
    "\n",
    "all_dfs = [df_30min, df_4h, df_daily]\n",
    "\n",
    "# Convert timestamp columns to datetime\n",
    "for df in all_dfs:\n",
    "    df['open_time'] = pd.to_datetime(df['open_time'])\n",
    "    df['close_time'] = pd.to_datetime(df['close_time'])\n",
    "\n",
    "# Define feature sets\n",
    "price_features = ['close', 'high', 'low', 'volume', 'quote_vol', 'count', 'buy_base', 'buy_quote']\n",
    "diff_features = ['close_diff', 'high_diff', 'low_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c824bfb3-163b-40fd-8a2f-fd3ff500b3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "    directional_acc = np.mean(np.sign(y_true[1:] - y_true[:-1]) == np.sign(y_pred[1:] - y_pred[:-1]))\n",
    "    return mse, mae, rmse, r2, mape, directional_acc\n",
    "\n",
    "def save_predictions(y_train, y_train_pred, y_test, y_test_pred, model_counter, file_prefix):\n",
    "    train_df = pd.DataFrame({\"Actual\": y_train, f\"Model_{model_counter}\": y_train_pred})\n",
    "    test_df = pd.DataFrame({\"Actual\": y_test, f\"Model_{model_counter}\": y_test_pred})\n",
    "    train_file_path = f\"{file_prefix}_train_predictions_model_{model_counter}.csv\"\n",
    "    test_file_path = f\"{file_prefix}_test_predictions_model_{model_counter}.csv\"\n",
    "    train_df.to_csv(train_file_path, index=False)\n",
    "    test_df.to_csv(test_file_path, index=False)\n",
    "\n",
    "def train_and_evaluate_arima(df, target_column, params, model_counter, file_prefix='results'):\n",
    "    p = params['p']\n",
    "    d = params['d']\n",
    "    q = params['q']\n",
    "\n",
    "    train_size = int(len(df) * 0.8)\n",
    "    train, test = df[target_column].values[:train_size], df[target_column].values[train_size:]\n",
    "\n",
    "    start_time = time.time()\n",
    "    model = ARIMA(train, order=(p, d, q))\n",
    "    model_fit = model.fit()\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "\n",
    "    predictions = model_fit.forecast(steps=len(test))\n",
    "    train_predictions = model_fit.fittedvalues\n",
    "\n",
    "    # Ensure that all predictions are aligned properly\n",
    "    if len(train) != len(train_predictions):\n",
    "        train = train[-len(train_predictions):]\n",
    "\n",
    "    train_mse, train_mae, train_rmse, train_r2, train_mape, train_directional_acc = evaluate_model(train, train_predictions)\n",
    "    test_mse, test_mae, test_rmse, test_r2, test_mape, test_directional_acc = evaluate_model(test, predictions)\n",
    "\n",
    "    result = {\n",
    "        \"p\": p,\n",
    "        \"d\": d,\n",
    "        \"q\": q,\n",
    "        \"train_mse\": train_mse,\n",
    "        \"test_mse\": test_mse,\n",
    "        \"train_mae\": train_mae,\n",
    "        \"test_mae\": test_mae,\n",
    "        \"train_rmse\": train_rmse,\n",
    "        \"test_rmse\": test_rmse,\n",
    "        \"train_r2\": train_r2,\n",
    "        \"test_r2\": test_r2,\n",
    "        \"train_mape\": train_mape,\n",
    "        \"test_mape\": test_mape,\n",
    "        \"train_directional_acc\": train_directional_acc,\n",
    "        \"test_directional_acc\": test_directional_acc,\n",
    "        \"training_time\": training_time\n",
    "    }\n",
    "\n",
    "    with open(f'{file_prefix}_{target_column}.csv', 'a', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=result.keys())\n",
    "        if f.tell() == 0:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(result)\n",
    "\n",
    "    save_predictions(train, train_predictions, test, predictions, model_counter, file_prefix)\n",
    "    \n",
    "    print(f\"Results: Train MAE: {train_mae:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "    print(f\"Train Directional Accuracy: {train_directional_acc:.4f}, Test Directional Accuracy: {test_directional_acc:.4f}\")\n",
    "    print(f\"Training Time: {training_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f733b51-b1c7-4cdc-b093-69d8c61f917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = 'close_diff'\n",
    "file_prefix = 'ARIMA_30min1'\n",
    "\n",
    "# Define parameter space\n",
    "parameter_space = {\n",
    "    'p': [0, 1, 2, 3, 4, 5],\n",
    "    'd': [0, 1, 2],\n",
    "    'q': [0, 1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "# Grid Search\n",
    "start = time.time()\n",
    "total_models = len(list(itertools.product(parameter_space['p'], parameter_space['d'], parameter_space['q'])))\n",
    "model_counter = 1\n",
    "\n",
    "for p, d, q in itertools.product(parameter_space['p'], parameter_space['d'], parameter_space['q']):\n",
    "    param_dict = {'p': p, 'd': d, 'q': q}\n",
    "    print(f\"Running model {model_counter}/{total_models} with parameters: {param_dict}\\n\")\n",
    "    train_and_evaluate_arima(df_30min, target_column, param_dict, model_counter, file_prefix)\n",
    "    model_counter += 1\n",
    "\n",
    "end = time.time()\n",
    "total_time = end - start\n",
    "print(f\"Total time taken: {total_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c767774e-3f3d-46c4-92a7-53227fceaf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_prefix = 'ARIMA_4h2'\n",
    "\n",
    "# Define parameter space\n",
    "parameter_space = {\n",
    "    'p': [0, 1, 2, 3, 4, 5],\n",
    "    'd': [0, 1, 2],\n",
    "    'q': [0, 1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "# Grid Search\n",
    "start = time.time()\n",
    "total_models = len(list(itertools.product(parameter_space['p'], parameter_space['d'], parameter_space['q'])))\n",
    "model_counter = 1\n",
    "\n",
    "for p, d, q in itertools.product(parameter_space['p'], parameter_space['d'], parameter_space['q']):\n",
    "    param_dict = {'p': p, 'd': d, 'q': q}\n",
    "    print(f\"Running model {model_counter}/{total_models} with parameters: {param_dict}\\n\")\n",
    "    train_and_evaluate_arima(df_4h, target_column, param_dict, model_counter, file_prefix)\n",
    "    model_counter += 1\n",
    "\n",
    "end = time.time()\n",
    "total_time = end - start\n",
    "print(f\"Total time taken: {total_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95edb2a7-e01e-4bb9-83ad-dcb5006fdde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter space\n",
    "parameter_space = {\n",
    "    'p': [0, 1, 2, 3],\n",
    "    'd': [0, 1, 2],\n",
    "    'q': [0, 1, 2, 3]\n",
    "}\n",
    "\n",
    "# Grid Search\n",
    "file_prefix = 'ARIMA_test2'\n",
    "start = time.time()\n",
    "total_models = len(list(itertools.product(parameter_space['p'], parameter_space['d'], parameter_space['q'])))\n",
    "model_counter = 1\n",
    "\n",
    "for p, d, q in itertools.product(parameter_space['p'], parameter_space['d'], parameter_space['q']):\n",
    "    param_dict = {'p': p, 'd': d, 'q': q}\n",
    "    print(f\"Running model {model_counter}/{total_models} with parameters: {param_dict}\\n\")\n",
    "    train_and_evaluate_arima(df_daily, target_column, param_dict, model_counter, file_prefix)\n",
    "    model_counter += 1\n",
    "\n",
    "end = time.time()\n",
    "total_time = end - start\n",
    "print(f\"Total time taken: {total_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e9ef3b-f6a5-4b9a-afda-4d50e82c0eab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d2d474-40e0-47f1-b486-a8ecc0807375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b685fe7-f63d-4007-baec-5483551ba784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c6abd5-a62f-4a90-ae9e-3a3212206bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb37cd0-f6b5-45ac-ab44-bebfd57aae71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
