{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d78028-a683-4015-a80d-ffb03834b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation and Preparation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Evaluation metrics and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "# Utility imports\n",
    "import itertools\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Set up GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07789e1a-65df-453c-b5d1-25d90374e047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed datasets\n",
    "path_30min = ''\n",
    "path_4h = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a5a503-ca1f-4613-b201-6aa0741b1f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_30min = pd.read_csv(path_30min)\n",
    "df_4h = pd.read_csv(path_4h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b000cfe1-bddd-4676-a42d-929642febd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = [df_30min, df_4h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc9439f-1b79-45dd-81b2-3a82b9cd8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp columns to datetime\n",
    "for df in all_dfs:\n",
    "    df['open_time'] = pd.to_datetime(df['open_time'])\n",
    "    df['close_time'] = pd.to_datetime(df['close_time'])\n",
    "\n",
    "# Define feature sets\n",
    "price_features = ['close', 'high', 'low', 'volume', 'quote_vol', 'count', 'buy_base', 'buy_quote']\n",
    "diff_features = ['close_diff', 'high_diff', 'low_diff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f752b-3693-4a58-95de-88df1a1a90c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, lstm_dim, dense_dim, output_dim, output_window_size, num_layers=1, dropout=0.0, activation_function=nn.ReLU):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, lstm_dim, num_layers=num_layers, batch_first=True, dropout=(dropout if num_layers > 1 else 0))\n",
    "        self.fc1 = nn.Linear(lstm_dim, dense_dim)\n",
    "        self.activation = activation_function()\n",
    "        self.fc2 = nn.Linear(dense_dim, output_dim * output_window_size)\n",
    "        self.output_window_size = output_window_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        return x.view(-1, self.output_window_size, 1)\n",
    "\n",
    "def create_sequences(data, target, input_window_size, output_window_size):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(data) - input_window_size - output_window_size + 1):\n",
    "        seq = data[i:i + input_window_size]\n",
    "        label = target[i + input_window_size:i + input_window_size + output_window_size]\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "def prepare_data(df, target_column, input_window_size, output_window_size, feature_columns):\n",
    "    X = df[feature_columns].values\n",
    "    y = df[target_column].values.reshape(-1, 1)\n",
    "    \n",
    "    scaler_X = MinMaxScaler()\n",
    "    X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "    scaler_y = MinMaxScaler()\n",
    "    y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, shuffle=False)\n",
    "    X_train_seq, y_train_seq = create_sequences(X_train, y_train, input_window_size, output_window_size)\n",
    "    X_test_seq, y_test_seq = create_sequences(X_test, y_test, input_window_size, output_window_size)\n",
    "\n",
    "    return X_train_seq, X_test_seq, y_train_seq, y_test_seq, scaler_y\n",
    "\n",
    "def plot_and_save_loss(training_loss, validation_loss, file_prefix, model_counter, start_epoch=3):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(start_epoch, len(training_loss)), training_loss[start_epoch:], label='Training Loss')\n",
    "    plt.plot(range(start_epoch, len(validation_loss)), validation_loss[start_epoch:], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Model {model_counter} - Training and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{file_prefix}_loss_plot_model_{model_counter}.png')\n",
    "    plt.close()\n",
    "\n",
    "    loss_df = pd.DataFrame({\n",
    "        \"Epoch\": range(len(training_loss)),\n",
    "        \"Training Loss\": training_loss,\n",
    "        \"Validation Loss\": validation_loss\n",
    "    })\n",
    "    loss_file_path = f'{file_prefix}_losses_model_{model_counter}.csv'\n",
    "    loss_df.to_csv(loss_file_path, index=False)\n",
    "\n",
    "def save_predictions(y_train, y_train_pred, y_test, y_test_pred, model_counter, file_prefix):\n",
    "    train_df = pd.DataFrame({\"Actual\": y_train.flatten(), f\"Model_{model_counter}\": y_train_pred.flatten()})\n",
    "    test_df = pd.DataFrame({\"Actual\": y_test.flatten(), f\"Model_{model_counter}\": y_test_pred.flatten()})\n",
    "\n",
    "    train_file_path = f\"{file_prefix}_train_predictions_model_{model_counter}.csv\"\n",
    "    test_file_path = f\"{file_prefix}_test_predictions_model_{model_counter}.csv\"\n",
    "    train_df.to_csv(train_file_path, index=False)\n",
    "    test_df.to_csv(test_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8103905e-15c4-40f3-930d-fd8334814c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(df, target_column, feature_columns, params, model_counter, file_prefix='results', use_early_stopping=False, plot_loss=False):\n",
    "    result = {}\n",
    "    try:\n",
    "        window_size = params['window_size']\n",
    "        output_window_size = params['output_window_size']\n",
    "        lstm_dim = params['lstm_dim']\n",
    "        dense_dim = params['dense_dim']\n",
    "        num_layers = params['num_layers']\n",
    "        dropout = params['dropout']\n",
    "        lr = params['lr']\n",
    "        batch_size = params['batch_size']\n",
    "        num_epochs = params['num_epochs']\n",
    "        optimizer_type = params['optimizer_type']\n",
    "        patience = params['patience']\n",
    "        activation_function = params['activation_function']\n",
    "\n",
    "        X_train_seq, X_test_seq, y_train_seq, y_test_seq, scaler_y = prepare_data(df, target_column, window_size, output_window_size, feature_columns)\n",
    "\n",
    "        X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32).to(device)\n",
    "        X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32).to(device)\n",
    "        y_train_tensor = torch.tensor(y_train_seq, dtype=torch.float32).to(device)\n",
    "        y_test_tensor = torch.tensor(y_test_seq, dtype=torch.float32).to(device)\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        input_dim = X_train_tensor.shape[2]\n",
    "        output_dim = 1\n",
    "\n",
    "        model = LSTMModel(input_dim, lstm_dim, dense_dim, output_dim, output_window_size, num_layers, dropout, activation_function).to(device)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = getattr(optim, optimizer_type)(model.parameters(), lr=lr)\n",
    "\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        training_losses = []\n",
    "        validation_losses = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            epoch_train_loss = 0\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_train_loss += loss.item()\n",
    "\n",
    "            epoch_train_loss /= len(train_loader)\n",
    "            training_losses.append(epoch_train_loss)\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in test_loader:\n",
    "                    outputs = model(inputs)\n",
    "                    val_loss += criterion(outputs, labels).item()\n",
    "\n",
    "            val_loss /= len(test_loader)\n",
    "            validation_losses.append(val_loss)\n",
    "\n",
    "            if use_early_stopping:\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                        break\n",
    "\n",
    "        training_time = time.time() - start_time\n",
    "\n",
    "        model.eval()\n",
    "        evaluation_start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            train_predictions = model(X_train_tensor).cpu().numpy()\n",
    "            test_predictions = model(X_test_tensor).cpu().numpy()\n",
    "\n",
    "            train_predictions_inverse = scaler_y.inverse_transform(train_predictions.reshape(-1, output_window_size))\n",
    "            test_predictions_inverse = scaler_y.inverse_transform(test_predictions.reshape(-1, output_window_size))\n",
    "            y_train_inverse = scaler_y.inverse_transform(y_train_seq.reshape(-1, output_window_size))\n",
    "            y_test_inverse = scaler_y.inverse_transform(y_test_seq.reshape(-1, output_window_size))\n",
    "\n",
    "            # Compute metrics for each step in the prediction window\n",
    "            train_mse = np.mean([mean_squared_error(y_train_inverse[:, i], train_predictions_inverse[:, i]) for i in range(output_window_size)])\n",
    "            test_mse = np.mean([mean_squared_error(y_test_inverse[:, i], test_predictions_inverse[:, i]) for i in range(output_window_size)])\n",
    "            train_mae = np.mean([mean_absolute_error(y_train_inverse[:, i], train_predictions_inverse[:, i]) for i in range(output_window_size)])\n",
    "            test_mae = np.mean([mean_absolute_error(y_test_inverse[:, i], test_predictions_inverse[:, i]) for i in range(output_window_size)])\n",
    "            train_rmse = np.sqrt(train_mse)\n",
    "            test_rmse = np.sqrt(test_mse)\n",
    "            train_r2 = np.mean([r2_score(y_train_inverse[:, i], train_predictions_inverse[:, i]) for i in range(output_window_size)])\n",
    "            test_r2 = np.mean([r2_score(y_test_inverse[:, i], test_predictions_inverse[:, i]) for i in range(output_window_size)])\n",
    "\n",
    "            train_mape = np.mean([mean_absolute_percentage_error(y_train_inverse[:, i], train_predictions_inverse[:, i]) * 100 for i in range(output_window_size)])\n",
    "            test_mape = np.mean([mean_absolute_percentage_error(y_test_inverse[:, i], test_predictions_inverse[:, i]) * 100 for i in range(output_window_size)])\n",
    "\n",
    "            train_directional_acc = np.mean([np.mean(np.sign(y_train_inverse[1:, i] - y_train_inverse[:-1, i]) == np.sign(train_predictions_inverse[1:, i] - train_predictions_inverse[:-1, i])) for i in range(output_window_size)])\n",
    "            test_directional_acc = np.mean([np.mean(np.sign(y_test_inverse[1:, i] - y_test_inverse[:-1, i]) == np.sign(test_predictions_inverse[1:, i] - test_predictions_inverse[:-1, i])) for i in range(output_window_size)])\n",
    "\n",
    "        evaluation_time = time.time() - evaluation_start_time\n",
    "\n",
    "        result = {\n",
    "            \"window_size\": window_size,\n",
    "            \"output_window_size\": output_window_size,\n",
    "            \"lstm_dim\": lstm_dim,\n",
    "            \"num_layers\": num_layers,\n",
    "            \"dense_dim\": dense_dim,\n",
    "            \"dropout\": dropout,\n",
    "            \"lr\": lr,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"optimizer_type\": optimizer_type,\n",
    "            \"train_mse\": train_mse,\n",
    "            \"test_mse\": test_mse,\n",
    "            \"train_mae\": train_mae,\n",
    "            \"test_mae\": test_mae,\n",
    "            \"train_rmse\": train_rmse,\n",
    "            \"test_rmse\": test_rmse,\n",
    "            \"train_r2\": train_r2,\n",
    "            \"test_r2\": test_r2,\n",
    "            \"train_mape\": train_mape,\n",
    "            \"test_mape\": test_mape,\n",
    "            \"train_directional_acc\": train_directional_acc,\n",
    "            \"test_directional_acc\": test_directional_acc,\n",
    "            \"training_time\": training_time,\n",
    "            \"evaluation_time\": evaluation_time,\n",
    "            \"patience\": patience,\n",
    "            \"activation_function\": activation_function.__name__\n",
    "        }\n",
    "\n",
    "        with open(f'{file_prefix}_{target_column}.csv', 'a', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=result.keys())\n",
    "            if f.tell() == 0:\n",
    "                writer.writeheader()\n",
    "            writer.writerow(result)\n",
    "\n",
    "        save_predictions(y_train_inverse, train_predictions_inverse, y_test_inverse, test_predictions_inverse, model_counter, file_prefix)\n",
    "\n",
    "        if plot_loss:\n",
    "            plot_and_save_loss(training_losses, validation_losses, file_prefix, model_counter)\n",
    "\n",
    "        print(f\"Results: Train MAE: {train_mae:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "        print(f\"Train Directional Accuracy: {train_directional_acc:.4f}, Test Directional Accuracy: {test_directional_acc:.4f}\")\n",
    "        print(f\"Training Time: {training_time:.4f} seconds, Evaluation Time: {evaluation_time:.4f} seconds\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        result = {\n",
    "            \"window_size\": window_size,\n",
    "            \"output_window_size\": output_window_size,\n",
    "            \"lstm_dim\": lstm_dim,\n",
    "            \"num_layers\": num_layers,\n",
    "            \"dense_dim\": dense_dim,\n",
    "            \"dropout\": dropout,\n",
    "            \"lr\": lr,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"optimizer_type\": optimizer_type,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7bb23f-4eec-4504-8139-9e2e5d6fea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_space = {\n",
    "    'window_size': [6],\n",
    "    'prediction_steps': [6],\n",
    "    'lstm_dim': [50, 80, 110, 140],\n",
    "    'dense_dim': [50, 80, 110, 140],\n",
    "    'num_layers': [1],\n",
    "    'dropout': [0.0],\n",
    "    'lr': [0.0001, 0.0002, 0.00005],\n",
    "    'num_epochs': [50, 80, 110, 140, 170, 200],\n",
    "    'batch_size': [32, 64, 96],\n",
    "    'optimizer_type': ['Adam'],\n",
    "    'patience': [24],\n",
    "    'activation_function': [nn.Tanh]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d5d6a9-a107-47d9-8be3-8f1c420eb6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search implementation\n",
    "target_column = 'close_diff'\n",
    "features = price_features + diff_features\n",
    "file_prefix='LSTM_4h_future_randomsearch'\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Generate random sample of parameter combinations\n",
    "parameter_combinations = list(itertools.product(*parameter_space.values()))\n",
    "random.shuffle(parameter_combinations)\n",
    "random_sample = parameter_combinations[:100]\n",
    "\n",
    "model_counter = 1\n",
    "\n",
    "for params in random_sample:\n",
    "    param_dict = dict(zip(parameter_space.keys(), params))\n",
    "    print(f\"Running model {model_counter}/{100} with parameters: {param_dict}\\n\")\n",
    "    \n",
    "    train_and_evaluate(df, target_column=target_column, feature_columns=features, params=param_dict, model_counter=model_counter, file_prefix=file_prefix, use_early_stopping=False, plot_loss=False)\n",
    "    \n",
    "    model_counter += 1\n",
    "\n",
    "end = time.time()\n",
    "total_time = end - start\n",
    "\n",
    "print(f\"Total time taken: {total_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fdd7b4-ebd1-47b7-a933-086f9cce9f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_space = {\n",
    "    'window_size': [6],\n",
    "    'output_window_size': [6],\n",
    "    'lstm_dim': [50, 80, 110, 140],\n",
    "    'dense_dim': [50, 80, 110, 140],\n",
    "    'num_layers': [1],\n",
    "    'dropout': [0.0],\n",
    "    'lr': [0.0001, 0.0002, 0.00005],\n",
    "    'num_epochs': [50, 80, 110, 140, 170, 200],\n",
    "    'batch_size': [32, 64, 96],\n",
    "    'optimizer_type': ['Adam'],\n",
    "    'patience': [24],\n",
    "    'activation_function': [nn.Tanh]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fe161e-4eec-47a7-8a8e-f6c0d887bc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search implementation\n",
    "target_column = 'close_diff'\n",
    "features = price_features + diff_features\n",
    "file_prefix='LSTM_4h_future_randomsearch'\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Generate random sample of parameter combinations\n",
    "parameter_combinations = list(itertools.product(*parameter_space.values()))\n",
    "random.shuffle(parameter_combinations)\n",
    "random_sample = parameter_combinations[:100]\n",
    "\n",
    "model_counter = 1\n",
    "\n",
    "for params in random_sample:\n",
    "    param_dict = dict(zip(parameter_space.keys(), params))\n",
    "    print(f\"Running model {model_counter}/{100} with parameters: {param_dict}\\n\")\n",
    "    \n",
    "    train_and_evaluate(df, target_column=target_column, feature_columns=features, params=param_dict, model_counter=model_counter, file_prefix=file_prefix, use_early_stopping=False, plot_loss=False)\n",
    "    \n",
    "    model_counter += 1\n",
    "\n",
    "end = time.time()\n",
    "total_time = end - start\n",
    "\n",
    "print(f\"Total time taken: {total_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396aac9e-fafd-4ff0-bd61-fa56ba575144",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_space = {\n",
    "    'window_size': [48],\n",
    "    'output_window_size': [48],\n",
    "    'lstm_dim': [30, 50, 80, 110, 140],\n",
    "    'dense_dim': [30, 50, 80, 110, 140],\n",
    "    'num_layers': [1],\n",
    "    'dropout': [0.0],\n",
    "    'lr': [0.0001, 0.00005],\n",
    "    'num_epochs': [50, 80, 110, 140, 170, 200],\n",
    "    'batch_size': [10, 20, 30, 40],\n",
    "    'optimizer_type': ['Adam'],\n",
    "    'patience': [24],\n",
    "    'activation_function': [nn.Tanh]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c64a62-561f-4063-9869-16ad51115ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random search implementation\n",
    "target_column = 'close_diff'\n",
    "features = price_features + diff_features\n",
    "file_prefix='LSTM_30min_future_randomsearch'\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Generate random sample of parameter combinations\n",
    "parameter_combinations = list(itertools.product(*parameter_space.values()))\n",
    "random.shuffle(parameter_combinations)\n",
    "random_sample = parameter_combinations[:150]\n",
    "\n",
    "model_counter = 1\n",
    "\n",
    "for params in random_sample:\n",
    "    param_dict = dict(zip(parameter_space.keys(), params))\n",
    "    print(f\"Running model {model_counter}/{150} with parameters: {param_dict}\\n\")\n",
    "    \n",
    "    train_and_evaluate(df, target_column=target_column, feature_columns=features, params=param_dict, model_counter=model_counter, file_prefix=file_prefix, use_early_stopping=False, plot_loss=False)\n",
    "    \n",
    "    model_counter += 1\n",
    "\n",
    "end = time.time()\n",
    "total_time = end - start\n",
    "\n",
    "print(f\"Total time taken: {total_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27cd30b-a054-4a7c-9a1d-1a4972aa8559",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
